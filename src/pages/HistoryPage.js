import React from 'react';
import { Link } from 'react-router-dom';
import './styles/HistoryPage.css';
import GlossaryPopup from '../components/GlossaryPopup';


function HistoryPage() {
  return (
    <div className="history-page">
      <header className="page-header">
        <h1>Історія <GlossaryPopup term="Deepfake"/>-технологій</h1>
        <p>Від лабораторного експерименту до технології, що змінила світ.</p>
      </header>

      <main className="page-content">
        <section className="intro">
        <p>
          Початки технології створення діпфейків можна простежити аж у 1990-х роках, коли дослідницькі групи здійснювали спроби моделювання облич на основі цифрових даних. Основною метою тодішніх експериментів було не створення ілюзії, а аналіз зображень для медичних, криміналістичних або технічних задач. Ці алгоритми функціонували в межах вузьких задач, із низькою точністю та відчутною залежністю від обчислювальних ресурсів.
        </p>
        <p>
          Ситуація докорінно змінилася з появою методів глибокого машинного навчання та архітектур <GlossaryPopup term="конволюційних нейронних мереж"/>. У поєднанні зі стрімким зростанням обчислювальної потужності це зробило можливим автоматичне відтворення облич, жестів та голосів на принципово новому рівні. Вперше з’явилася можливість не лише аналізувати, а й генерувати — із вражаючою правдоподібністю.
        </p>
        </section>

        <section className="evolution">
          <h2>Еволюція діпфейків</h2>
          <p>
            Протягом першого десятиліття XXI століття більшість відповідних досліджень залишались в межах академічних лабораторій. Лише у 2017 році, із появою перших публічних інструментів, поняття «deepfake» отримало популяризацію в масовій культурі. Платформи на кшталт <GlossaryPopup term="Reddit"/> стали майданчиками для обміну алгоритмами, скриптами й тренувальними наборами.
          </p>
          <p>
            Поширення інструментів відбулося з надзвичайною швидкістю. Алгоритми, що початково вимагали технічної підготовки, незабаром стали доступними навіть користувачам без спеціалізованих знань. У відповідь — поява масових фейкових відео, включаючи політичні заяви, фальсифіковані виступи та візуалізації за участі публічних осіб. Дипфейки поступово перетворилися на інструмент маніпуляції та симуляції візуальної достовірності.
          </p>
        </section>

        <section className="media-response">
          <h2>Реакція медіа та інституцій</h2>
          <p>
            Спочатку журналістська спільнота розглядала діпфейки як курйоз або маргінальне явище. Ставлення змінилось із поширенням відео, в яких фальсифіковано виступи відомих політичних діячів, що потенційно могли вплинути на громадську думку та міжнародні відносини. У відповідь медіаорганізації почали впроваджувати нові протоколи верифікації матеріалів, зокрема аналіз метаданих, перевірку джерел та використання програм розпізнавання фальсифікацій.
          </p>
          <p>
            Були також ініційовані освітні кампанії з цифрової гігієни: громадянам рекомендували критично ставитися до візуального контенту, незалежно від його переконливої зовнішньої форми. Соціальні мережі додали індикатори змінення відео та позначення штучно створених матеріалів.
          </p>
        </section>

        <section className="milestones">
          <h2>Ключові дати</h2>
            <ul>
              <li><strong>2014</strong> — Представлено архітектуру <GlossaryPopup term="Generative Adversarial Networks"/> (GAN), яка відкрила можливості генеративного моделювання образів.</li>
              <li><strong>2017</strong> — З’являється <GlossaryPopup term="DeepFake App"/>, перший приклад доступного інструменту глибокої підміни облич у відео.</li>
              <li><strong>2018</strong> — Активний розвиток технологій синтезу мовлення: поява алгоритмів, здатних імітувати людську інтонацію, паузи та навіть логічні похибки.</li>
              <li><strong>2019</strong> — Розробка First Order Motion Model, що дала змогу оживлювати статичні зображення шляхом перенесення динаміки з відео-джерела.</li>
              <li><strong>2021</strong> — Інтеграція мультимодальних генераторів, що поєднують зображення, аудіо та текст в уніфіковані симульовані особистості.</li>
              <li><strong>2023</strong> — Застосування трансформерних моделей у реальному часі, що дозволило реалізовувати діпфейк-симуляцію під час онлайн-стрімів та відеоконференцій.</li>
            </ul>
        </section>

        <section className="future">
          <h2>Майбутнє deepfake-технологій  </h2>
          <p>
            У найближчому майбутньому технології діпфейків, ймовірно, продовжать еволюцію в бік дедалі глибшої інтеграції з іншими штучно-інтелектуальними системами. Розробка персоналізованих віртуальних агентів, вірогідно, стане масовим явищем, з урахуванням етичних протоколів та правових обмежень.
          </p>
          <p>
            Водночас очікується поглиблення нормативного регулювання, створення міжнародних ініціатив для боротьби з дезінформацією, а також перегляд підходів до ідентифікації цифрових суб’єктів. У новій інформаційній екосистемі автентичність більше не буде апріорною ознакою візуального матеріалу — вона потребуватиме підтвердження, а інколи й доведення.
          </p>
        </section>
      </main>

      <footer className="page-footer">
        <Link to="/" className="back-link">Повернутися на головну сторінку</Link>
      </footer>
    </div>
  );
}

export default HistoryPage;
